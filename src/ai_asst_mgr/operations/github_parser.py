"""Parse GitHub commits and detect AI vendor attribution.

This module provides functionality to parse git commits from repositories
and detect AI vendor signatures in commit messages (Claude, Gemini, OpenAI).
"""

from __future__ import annotations

import re
import subprocess
from dataclasses import dataclass
from datetime import UTC, datetime
from typing import TYPE_CHECKING, ClassVar

if TYPE_CHECKING:
    from collections.abc import Sequence
    from pathlib import Path

# Git log format: %H|%an|%ae|%aI|%s|%D|%b
# sha|author_name|author_email|date|subject|refs|body
GIT_LOG_FORMAT = "%H|%an|%ae|%aI|%s|%D"
GIT_LOG_SEP = "|"
# For body, we need a separate call since body can contain |

# Minimum required fields in git log output: sha, author_name, author_email, date, subject
MIN_GIT_LOG_PARTS = 5
# Maximum parts to split (6 parts: sha, name, email, date, subject, refs)
MAX_GIT_LOG_SPLIT = 5


@dataclass
class GitHubCommit:
    """Represents a GitHub commit with vendor attribution.

    Attributes:
        sha: The commit SHA hash.
        repo: Repository identifier (e.g., 'user/repo' or absolute path).
        branch: Branch name where commit was made.
        message: Full commit message.
        author_name: Git author name.
        author_email: Git author email.
        vendor_id: Detected AI vendor ('claude', 'gemini', 'openai', or None).
        committed_at: Commit timestamp as datetime.
    """

    sha: str
    repo: str
    branch: str | None
    message: str
    author_name: str | None
    author_email: str | None
    vendor_id: str | None
    committed_at: datetime

    @property
    def short_sha(self) -> str:
        """Return short (7-char) SHA."""
        return self.sha[:7]

    @property
    def is_ai_attributed(self) -> bool:
        """Check if commit is attributed to any AI vendor."""
        return self.vendor_id is not None


@dataclass
class Attribution:
    """Represents vendor attribution for a GitHub resource.

    Attributes:
        resource_type: Type of resource (commit, issue, pr, comment).
        resource_id: Identifier (SHA for commits, number for issues/PRs).
        repo: Repository identifier.
        vendor_id: Detected AI vendor ID or None if manual/unknown.
        confidence: Detection confidence score from 0.0 to 1.0.
        detection_method: How attribution was detected (signature, co_author, etc).
        detected_at: Timestamp when attribution was detected.
    """

    resource_type: str  # commit, issue, pr, comment
    resource_id: str  # SHA for commits, number for issues/PRs
    repo: str
    vendor_id: str | None  # None if manual/unknown
    confidence: float  # 0.0-1.0
    detection_method: str  # signature, co_author, email_domain, heuristic
    detected_at: datetime


# Vendor signature patterns for attribution detection
VENDOR_SIGNATURES: dict[str, list[re.Pattern[str]]] = {
    "claude": [
        re.compile(r"Generated with \[Claude Code\]", re.IGNORECASE),
        re.compile(r"Co-Authored-By: Claude", re.IGNORECASE),
        re.compile(r"Generated by Claude", re.IGNORECASE),
        re.compile(r"anthropic\.com", re.IGNORECASE),
        re.compile(r"ðŸ¤–.*Claude", re.IGNORECASE),
        re.compile(r"claude-code", re.IGNORECASE),
    ],
    "gemini": [
        re.compile(r"Generated by Gemini", re.IGNORECASE),
        re.compile(r"Co-Authored-By: Gemini", re.IGNORECASE),
        re.compile(r"google\.com.*gemini", re.IGNORECASE),
        re.compile(r"Gemini CLI", re.IGNORECASE),
        re.compile(r"gemini-cli", re.IGNORECASE),
    ],
    "openai": [
        re.compile(r"Generated by OpenAI", re.IGNORECASE),
        re.compile(r"Generated by Codex", re.IGNORECASE),
        re.compile(r"Co-Authored-By: OpenAI", re.IGNORECASE),
        re.compile(r"openai\.com", re.IGNORECASE),
        re.compile(r"ChatGPT", re.IGNORECASE),
        re.compile(r"GPT-4", re.IGNORECASE),
    ],
}

# Email domain patterns for vendor detection
VENDOR_EMAIL_DOMAINS: dict[str, list[str]] = {
    "claude": ["anthropic.com", "claude.ai"],
    "gemini": ["google.com"],
    "openai": ["openai.com"],
}


def detect_vendor_attribution_detailed(
    content: str,
    author_email: str | None = None,
) -> tuple[str | None, float, str]:
    """Detect AI vendor with confidence score and detection method.

    Performs multi-level detection with prioritized checks:
    1. Co-author patterns (confidence 0.95) - checked first for specificity
    2. Generation/signature patterns (confidence 1.0)
    3. Email domain patterns (confidence 0.8)

    Args:
        content: Text content to search (commit message, PR body, etc.)
        author_email: Optional email for domain-based detection

    Returns:
        Tuple of (vendor_id, confidence, detection_method)
        vendor_id is None if no attribution found
    """
    # Check for co-author patterns first (high confidence, most specific)
    co_author_patterns = {
        "claude": re.compile(r"Co-Authored-By:\s*Claude", re.IGNORECASE),
        "gemini": re.compile(r"Co-Authored-By:\s*Gemini", re.IGNORECASE),
        "openai": re.compile(r"Co-Authored-By:\s*OpenAI", re.IGNORECASE),
    }

    for vendor_id, pattern in co_author_patterns.items():
        if pattern.search(content):
            return (vendor_id, 0.95, "co_author")

    # Check for explicit generation/signature patterns (highest confidence)
    signature_patterns = {
        "claude": [
            re.compile(r"Generated with \[Claude Code\]", re.IGNORECASE),
            re.compile(r"Generated by Claude", re.IGNORECASE),
            re.compile(r"ðŸ¤–.*Claude", re.IGNORECASE),
            re.compile(r"claude-code", re.IGNORECASE),
            re.compile(r"anthropic\.com", re.IGNORECASE),
        ],
        "gemini": [
            re.compile(r"Generated by Gemini", re.IGNORECASE),
            re.compile(r"Gemini CLI", re.IGNORECASE),
            re.compile(r"gemini-cli", re.IGNORECASE),
            re.compile(r"google\.com.*gemini", re.IGNORECASE),
        ],
        "openai": [
            re.compile(r"Generated by OpenAI", re.IGNORECASE),
            re.compile(r"Generated by Codex", re.IGNORECASE),
            re.compile(r"ChatGPT", re.IGNORECASE),
            re.compile(r"GPT-4", re.IGNORECASE),
            re.compile(r"openai\.com", re.IGNORECASE),
        ],
    }

    for vendor_id, patterns in signature_patterns.items():
        for pattern in patterns:
            if pattern.search(content):
                return (vendor_id, 1.0, "signature")

    # Check email domain patterns (moderate confidence)
    if author_email:
        email_lower = author_email.lower()
        for vendor_id, domains in VENDOR_EMAIL_DOMAINS.items():
            for domain in domains:
                if domain in email_lower:
                    return (vendor_id, 0.8, "email_domain")

    # No attribution found
    return (None, 0.0, "none")


def detect_vendor_attribution(message: str) -> str | None:
    """Detect AI vendor from commit message signatures.

    Searches the commit message for known vendor signatures and returns
    the first vendor ID matched. This is a backward-compatible wrapper
    around detect_vendor_attribution_detailed().

    Args:
        message: Full commit message (subject + body).

    Returns:
        Vendor ID ('claude', 'gemini', 'openai') or None if no attribution found.
    """
    vendor_id, _, _ = detect_vendor_attribution_detailed(message)
    return vendor_id


def create_attribution(
    resource_type: str,
    resource_id: str,
    repo: str,
    content: str,
    author_email: str | None = None,
) -> Attribution:
    """Create an Attribution object from content analysis.

    Args:
        resource_type: Type of resource (commit, issue, pr, comment)
        resource_id: Resource identifier (SHA, issue/PR number, etc.)
        repo: Repository identifier
        content: Text content to analyze for attribution
        author_email: Optional email address for domain-based detection

    Returns:
        Attribution object with detected vendor and confidence score
    """
    vendor_id, confidence, method = detect_vendor_attribution_detailed(
        content,
        author_email,
    )

    return Attribution(
        resource_type=resource_type,
        resource_id=resource_id,
        repo=repo,
        vendor_id=vendor_id,
        confidence=confidence,
        detection_method=method,
        detected_at=datetime.now(UTC),
    )


def detect_pr_attribution(
    title: str,
    body: str,
    commits: list[str] | None = None,
) -> tuple[str | None, float, str]:
    """Detect vendor from PR title, body, and associated commits.

    Performs multi-source detection with prioritized checks:
    1. PR body content (highest priority)
    2. PR title
    3. Aggregated commit messages (if provided)

    The highest confidence detection across all sources is returned.

    Args:
        title: PR title text
        body: PR body/description text
        commits: Optional list of commit messages from the PR

    Returns:
        Tuple of (vendor_id, confidence, detection_method)
        vendor_id is None if no attribution found
    """
    # Check PR body first (most likely to have attribution)
    vendor_id, confidence, method = detect_vendor_attribution_detailed(body)
    if vendor_id is not None:
        return (vendor_id, confidence, method)

    # Check PR title
    vendor_id, confidence, method = detect_vendor_attribution_detailed(title)
    if vendor_id is not None:
        return (vendor_id, confidence, method)

    # Check commit messages if provided
    if commits:
        # Aggregate all commit messages for detection
        combined_commits = "\n\n".join(commits)
        vendor_id, confidence, method = detect_vendor_attribution_detailed(combined_commits)
        if vendor_id is not None:
            # Lower confidence slightly since it's from commits, not PR body
            return (vendor_id, max(0.7, confidence - 0.1), f"commit_{method}")

    # No attribution found
    return (None, 0.0, "none")


class GitLogParser:
    """Parse git log output into GitHubCommit objects.

    This class provides methods to extract commits from local git repositories
    with optional filtering by date range, branch, and other criteria.

    Example:
        >>> parser = GitLogParser()
        >>> commits = parser.parse_repo(Path("/path/to/repo"))
        >>> for commit in commits:
        ...     if commit.is_ai_attributed:
        ...         print(f"{commit.short_sha}: {commit.vendor_id}")
    """

    # Maximum commits to parse in one operation (safety limit)
    MAX_COMMITS: ClassVar[int] = 10000

    def parse_repo(
        self,
        repo_path: Path,
        since: datetime | None = None,
        until: datetime | None = None,
        branch: str | None = None,
        limit: int | None = None,
    ) -> list[GitHubCommit]:
        """Parse commits from a git repository.

        Args:
            repo_path: Path to the git repository root.
            since: Only include commits after this date.
            until: Only include commits before this date.
            branch: Specific branch to parse (default: current branch).
            limit: Maximum number of commits to return.

        Returns:
            List of GitHubCommit objects, newest first.

        Raises:
            ValueError: If repo_path is not a git repository.
            subprocess.SubprocessError: If git command fails.
        """
        if not self._is_git_repo(repo_path):
            msg = f"Not a git repository: {repo_path}"
            raise ValueError(msg)

        # Build git log command
        cmd = self._build_git_log_command(since, until, branch, limit)

        # Run git log
        result = subprocess.run(
            cmd,
            cwd=repo_path,
            capture_output=True,
            text=True,
            check=True,
            timeout=60,
        )

        # Parse output
        repo_name = self._get_repo_name(repo_path)
        return self._parse_git_log_output(result.stdout, repo_name)

    def parse_multiple_repos(
        self,
        repo_paths: Sequence[Path],
        since: datetime | None = None,
        until: datetime | None = None,
    ) -> list[GitHubCommit]:
        """Parse commits from multiple repositories.

        Args:
            repo_paths: Sequence of paths to git repositories.
            since: Only include commits after this date.
            until: Only include commits before this date.

        Returns:
            Combined list of commits from all repos, sorted by date (newest first).
        """
        all_commits: list[GitHubCommit] = []

        for repo_path in repo_paths:
            try:
                commits = self.parse_repo(repo_path, since=since, until=until)
                all_commits.extend(commits)
            except (ValueError, subprocess.SubprocessError):
                # Skip invalid repos
                continue

        # Sort by committed_at, newest first
        all_commits.sort(key=lambda c: c.committed_at, reverse=True)
        return all_commits

    def _is_git_repo(self, path: Path) -> bool:
        """Check if path is a git repository."""
        try:
            subprocess.run(
                ["git", "rev-parse", "--git-dir"],
                cwd=path,
                capture_output=True,
                check=True,
                timeout=10,
            )
        except (subprocess.SubprocessError, FileNotFoundError):
            return False
        else:
            return True

    def _build_git_log_command(
        self,
        since: datetime | None,
        until: datetime | None,
        branch: str | None,
        limit: int | None,
    ) -> list[str]:
        """Build the git log command with appropriate arguments."""
        cmd = [
            "git",
            "log",
            f"--format={GIT_LOG_FORMAT}%n%b%n---COMMIT_END---",
        ]

        if since:
            cmd.append(f"--since={since.isoformat()}")
        if until:
            cmd.append(f"--until={until.isoformat()}")
        if branch:
            cmd.append(branch)

        # Apply limit (with safety cap)
        effective_limit = min(limit or self.MAX_COMMITS, self.MAX_COMMITS)
        cmd.append(f"-{effective_limit}")

        return cmd

    def _get_repo_name(self, repo_path: Path) -> str:
        """Get repository name from path or remote."""
        try:
            # Try to get remote URL
            result = subprocess.run(
                ["git", "config", "--get", "remote.origin.url"],
                cwd=repo_path,
                capture_output=True,
                text=True,
                check=True,
                timeout=10,
            )
            remote_url = result.stdout.strip()
            # Extract repo name from URL
            # git@github.com:user/repo.git -> user/repo
            # https://github.com/user/repo.git -> user/repo
            if ":" in remote_url and "@" in remote_url:
                # SSH format
                repo_name = remote_url.split(":")[-1]
            else:
                # HTTPS format
                repo_name = "/".join(remote_url.split("/")[-2:])
            return repo_name.removesuffix(".git")
        except subprocess.SubprocessError:
            # Fall back to directory name
            return repo_path.name

    def _parse_git_log_output(self, output: str, repo_name: str) -> list[GitHubCommit]:
        """Parse raw git log output into GitHubCommit objects."""
        commits: list[GitHubCommit] = []

        # Split by commit delimiter
        commit_blocks = output.split("---COMMIT_END---")

        for raw_block in commit_blocks:
            block = raw_block.strip()
            if not block:
                continue

            try:
                commit = self._parse_commit_block(block, repo_name)
                if commit:
                    commits.append(commit)
            except (ValueError, IndexError):
                # Skip malformed commits
                continue

        return commits

    def _parse_commit_block(self, block: str, repo_name: str) -> GitHubCommit | None:
        """Parse a single commit block into a GitHubCommit."""
        lines = block.split("\n")
        if not lines:
            return None

        # First line is the formatted header
        header = lines[0]
        parts = header.split(GIT_LOG_SEP, MAX_GIT_LOG_SPLIT)
        if len(parts) < MIN_GIT_LOG_PARTS:
            return None

        sha = parts[0]
        author_name = parts[1] or None
        author_email = parts[2] or None
        date_str = parts[3]
        subject = parts[4]
        refs = parts[MIN_GIT_LOG_PARTS] if len(parts) > MIN_GIT_LOG_PARTS else ""

        # Rest is the body
        body = "\n".join(lines[1:]).strip()
        full_message = f"{subject}\n\n{body}".strip()

        # Parse date
        committed_at = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
        if committed_at.tzinfo is None:
            committed_at = committed_at.replace(tzinfo=UTC)

        # Extract branch from refs (e.g., "HEAD -> main, origin/main")
        branch = self._extract_branch_from_refs(refs)

        # Detect vendor attribution
        vendor_id = detect_vendor_attribution(full_message)

        return GitHubCommit(
            sha=sha,
            repo=repo_name,
            branch=branch,
            message=full_message,
            author_name=author_name,
            author_email=author_email,
            vendor_id=vendor_id,
            committed_at=committed_at,
        )

    def _extract_branch_from_refs(self, refs: str) -> str | None:
        """Extract branch name from git refs string."""
        if not refs:
            return None

        # Look for "HEAD -> branch" pattern
        if "HEAD -> " in refs:
            for ref in refs.split(", "):
                if ref.startswith("HEAD -> "):
                    return ref.removeprefix("HEAD -> ")

        # Look for "origin/branch" pattern
        for ref in refs.split(", "):
            if ref.startswith("origin/"):
                return ref.removeprefix("origin/")

        return None


def parse_git_log(
    repo_path: Path,
    since: datetime | None = None,
    until: datetime | None = None,
    limit: int | None = None,
) -> list[GitHubCommit]:
    """Parse commits directly from a git repository.

    Convenience function that creates a GitLogParser and parses a single repo.

    Args:
        repo_path: Path to the git repository root.
        since: Only include commits after this date.
        until: Only include commits before this date.
        limit: Maximum number of commits to return.

    Returns:
        List of GitHubCommit objects, newest first.
    """
    parser = GitLogParser()
    return parser.parse_repo(repo_path, since=since, until=until, limit=limit)


def find_git_repos(
    base_path: Path,
    max_depth: int = 3,
) -> list[Path]:
    """Find git repositories under a base path.

    Args:
        base_path: Base directory to search.
        max_depth: Maximum directory depth to search.

    Returns:
        List of paths to git repository roots.
    """
    repos: list[Path] = []

    def _search(path: Path, depth: int) -> None:
        if depth > max_depth:
            return

        if (path / ".git").is_dir():
            repos.append(path)
            return  # Don't recurse into nested repos

        try:
            for child in path.iterdir():
                if child.is_dir() and not child.name.startswith("."):
                    _search(child, depth + 1)
        except PermissionError:
            pass

    _search(base_path, 0)
    return repos
